{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00e89be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.20.1-cp310-cp310-win_amd64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (1.23.5)\n",
      "Collecting torch==2.5.1 (from torchvision)\n",
      "  Downloading torch-2.5.1-cp310-cp310-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.5.1->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.5.1->torchvision) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from torch==2.5.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch==2.5.1->torchvision) (2.1.1)\n",
      "Downloading torchvision-0.20.1-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 1.6/1.6 MB 5.5 MB/s eta 0:00:00\n",
      "Downloading torch-2.5.1-cp310-cp310-win_amd64.whl (203.1 MB)\n",
      "   ---------------------------------------- 203.1/203.1 MB 1.8 MB/s eta 0:00:00\n",
      "Installing collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.0\n",
      "    Uninstalling torch-2.5.0:\n",
      "      Successfully uninstalled torch-2.5.0\n",
      "Successfully installed torch-2.5.1 torchvision-0.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d6a4054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import OxfordIIITPet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f302d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcess:\n",
    "    def __init__(self, resize=(128, 128)):\n",
    "        self.resize = resize\n",
    "        # Image transformations\n",
    "        self.transform_image = transforms.Compose([\n",
    "            transforms.Resize(self.resize),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        # Mask transformations\n",
    "        self.transform_mask = transforms.Compose([\n",
    "            transforms.Resize(self.resize, interpolation=Image.NEAREST),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def load_oxford_pet(self, root='./data'):\n",
    "        dataset = OxfordIIITPet(\n",
    "            root=root,\n",
    "            download=True,\n",
    "            target_types=['category', 'segmentation']\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "    def preprocess_oxford_pet(self, dataset):\n",
    "        # Initialize lists to store processed data\n",
    "        processed_images = []\n",
    "        processed_masks = []\n",
    "        processed_labels = []\n",
    "\n",
    "        print(\"Processing dataset...\")\n",
    "        for idx, (img, targets) in enumerate(dataset):\n",
    "            try:\n",
    "                # Apply transformations\n",
    "                transformed_img = self.transform_image(img)\n",
    "                transformed_mask = self.transform_mask(targets[1])  # targets[1] is the segmentation mask\n",
    "                label = targets[0]  # targets[0] is the category label\n",
    "\n",
    "                # Convert to numpy and append\n",
    "                processed_images.append(transformed_img.numpy())\n",
    "                processed_masks.append(transformed_mask.numpy())\n",
    "                processed_labels.append(label)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {idx}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        processed_images = np.array(processed_images)\n",
    "        processed_masks = np.array(processed_masks)\n",
    "        processed_labels = np.array(processed_labels)\n",
    "\n",
    "        # Split the data\n",
    "        train_idx, test_idx = train_test_split(\n",
    "            np.arange(len(processed_labels)),\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=processed_labels\n",
    "        )\n",
    "\n",
    "        # Create train and test sets\n",
    "        train_images = processed_images[train_idx]\n",
    "        test_images = processed_images[test_idx]\n",
    "        train_masks = processed_masks[train_idx]\n",
    "        test_masks = processed_masks[test_idx]\n",
    "        train_labels = processed_labels[train_idx]\n",
    "        test_labels = processed_labels[test_idx]\n",
    "\n",
    "        return (train_images, test_images,\n",
    "                train_labels, test_labels,\n",
    "                train_masks, test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "091c1f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetDataset(Dataset):\n",
    "    def __init__(self, images, masks, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (np.ndarray): Image data with shape (N, C, H, W)\n",
    "            masks (np.ndarray): Mask data with shape (N, 1, H, W)\n",
    "            labels (np.ndarray): Label data with shape (N,)\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert numpy arrays to tensors\n",
    "        image = torch.from_numpy(self.images[idx]).float()\n",
    "        mask = torch.from_numpy(self.masks[idx]).float()\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return image, mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f209aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Classes\n",
    "\n",
    "# CNN for Classification\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=37):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 512)  # Adjust these numbers based on your input size\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 16 * 16)  # Adjust these numbers based on your input size\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# U-Net for Segmentation\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = self.conv_block(3, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec1 = self.conv_block(512, 256)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = self.conv_block(256, 128)\n",
    "        self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec3 = self.conv_block(128, 64)\n",
    "\n",
    "        self.final = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        return block\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(nn.MaxPool2d(2,2)(enc1))\n",
    "        enc3 = self.enc3(nn.MaxPool2d(2,2)(enc2))\n",
    "        enc4 = self.enc4(nn.MaxPool2d(2,2)(enc3))\n",
    "\n",
    "        dec1 = self.up1(enc4)\n",
    "        dec1 = torch.cat((dec1, enc3), dim=1)\n",
    "        dec1 = self.dec1(dec1)\n",
    "\n",
    "        dec2 = self.up2(dec1)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.dec2(dec2)\n",
    "\n",
    "        dec3 = self.up3(dec2)\n",
    "        dec3 = torch.cat((dec3, enc1), dim=1)\n",
    "        dec3 = self.dec3(dec3)\n",
    "\n",
    "        out = self.final(dec3)\n",
    "        return out\n",
    "\n",
    "# Recurrent Neural Networks\n",
    "\n",
    "# Simple RNN\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size=30, hidden_size=64, num_layers=2):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, nonlinearity='tanh', batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# LSTM\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=30, hidden_size=64, num_layers=2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Transformer\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size=30, nhead=4, num_layers=2, hidden_dim=128):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=input_size,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=hidden_dim\n",
    "        )\n",
    "        self.fc = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        out = self.transformer(src, tgt)\n",
    "        out = self.fc(out[-1,:,:])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd95f51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.3: Training and Evaluation\n",
    "def train_model(model, train_loader, criterion, optimizer, device, epochs=5, early_stopping_patience=5):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (images, masks, labels) in enumerate(train_loader):\n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)  # We only need labels for classification\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Batch [{batch_idx+1}/{len(train_loader)}], '\n",
    "                      f'Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Early stopping check\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "            break\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] Loss: {epoch_loss:.4f}')\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device, task_type='classification'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "404b259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Training and Evaluation for U-Net and Recurrent Models\n",
    "\n",
    "def train_unet(model, train_loader, criterion, optimizer, device, epochs=5, early_stopping_patience=5):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (images, masks, _) in enumerate(train_loader):  # Ignore labels\n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Batch [{batch_idx+1}/{len(train_loader)}], '\n",
    "                      f'Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Early stopping check\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "            break\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] Loss: {epoch_loss:.4f}')\n",
    "\n",
    "def evaluate_unet(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks, _ in test_loader:  # Ignore labels\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    print(f'Test Loss: {avg_loss:.4f}')\n",
    "    return avg_loss\n",
    "\n",
    "def dice_coef(pred, mask):\n",
    "    pred = pred.astype(bool)\n",
    "    mask = mask.astype(bool)\n",
    "    intersection = np.logical_and(pred, mask).sum()\n",
    "    union = pred.sum() + mask.sum()\n",
    "    if union == 0:\n",
    "        return 1.0\n",
    "    return 2. * intersection / union\n",
    "\n",
    "def train_regression_model(model, dataloader, criterion, optimizer, device, epochs=5, early_stopping_patience=5):\n",
    "    model.to(device)\n",
    "    best_loss = float('inf')\n",
    "    patience = early_stopping_patience\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for sequences, targets in dataloader:\n",
    "            sequences = sequences.to(device).unsqueeze(-1)  # Add feature dimension\n",
    "            targets = targets.to(device).unsqueeze(-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            patience = early_stopping_patience\n",
    "            # Save the best model if needed\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "def evaluate_regression_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in dataloader:\n",
    "            sequences = sequences.to(device).unsqueeze(-1)\n",
    "            targets = targets.to(device).unsqueeze(-1)\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    mae = np.mean(np.abs(np.array(all_preds) - np.array(all_targets)))\n",
    "    rmse = np.sqrt(np.mean((np.array(all_preds) - np.array(all_targets))**2))\n",
    "    print(f'Loss: {epoch_loss:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2823779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Training and Evaluation\n",
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = torch.tensor(self.sequences[idx], dtype=torch.float32).unsqueeze(1)  # (seq_len, feature)\n",
    "        tgt = torch.tensor([self.targets[idx]], dtype=torch.float32)  # (1)\n",
    "        return src, tgt\n",
    "\n",
    "def train_transformer(model, dataloader, criterion, optimizer, device, epochs=5, early_stopping_patience=5):\n",
    "    model.to(device)\n",
    "    best_loss = float('inf')\n",
    "    patience = early_stopping_patience\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for src, tgt in dataloader:\n",
    "            src = src.to(device).permute(1, 0, 2)  # Transformer expects (seq_len, batch, feature)\n",
    "            tgt_input = src  # For simplicity, using src as tgt_input\n",
    "            tgt = tgt.to(device).unsqueeze(0)  # (1, batch, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(src, tgt_input)\n",
    "            loss = criterion(outputs, tgt)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            patience = early_stopping_patience\n",
    "            # Save the best model if needed\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "def evaluate_transformer(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src = src.to(device).permute(1, 0, 2)\n",
    "            tgt_input = src\n",
    "            tgt = tgt.to(device).unsqueeze(0)\n",
    "            outputs = model(src, tgt_input)\n",
    "            loss = criterion(outputs, tgt)\n",
    "            running_loss += loss.item()\n",
    "            all_preds.extend(outputs.cpu().numpy().flatten())\n",
    "            all_targets.extend(tgt.cpu().numpy().flatten())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    mae = np.mean(np.abs(np.array(all_preds) - np.array(all_targets)))\n",
    "    rmse = np.sqrt(np.mean((np.array(all_preds) - np.array(all_targets))**2))\n",
    "    print(f'Loss: {epoch_loss:.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}')\n",
    "\n",
    "# Visualization Functions\n",
    "def visualize_training_history(history, metric='loss'):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(history['train'], label='Train')\n",
    "    plt.plot(history['val'], label='Validation')\n",
    "    plt.title(f'Training and Validation {metric.capitalize()}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_segmentation_results(model, dataloader, device, num_samples=5):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, masks, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            for i in range(inputs.size(0)):\n",
    "                if samples >= num_samples:\n",
    "                    return\n",
    "                img = inputs[i].cpu().permute(1, 2, 0).numpy()\n",
    "                img = (img * 0.5) + 0.5  # Denormalize\n",
    "                mask = masks[i].cpu().squeeze().numpy()\n",
    "                pred = preds[i].cpu().squeeze().numpy()\n",
    "\n",
    "                fig, axs = plt.subplots(1,3, figsize=(15,5))\n",
    "                axs[0].imshow(img)\n",
    "                axs[0].set_title('Input Image')\n",
    "                axs[1].imshow(mask, cmap='gray')\n",
    "                axs[1].set_title('Ground Truth Mask')\n",
    "                axs[2].imshow(pred, cmap='gray')\n",
    "                axs[2].set_title('Predicted Mask')\n",
    "                plt.show()\n",
    "                samples += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0a6a669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_3844\\3816544364.py:13: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  transforms.Resize(self.resize, interpolation=Image.NEAREST),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Oxford-IIIT Pet Dataset...\n",
      "Downloading https://thor.robots.ox.ac.uk/pets/images.tar.gz to data\\oxford-iiit-pet\\images.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 792M/792M [02:43<00:00, 4.84MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\oxford-iiit-pet\\images.tar.gz to data\\oxford-iiit-pet\n",
      "Downloading https://thor.robots.ox.ac.uk/pets/annotations.tar.gz to data\\oxford-iiit-pet\\annotations.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 19.2M/19.2M [00:10<00:00, 1.80MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\oxford-iiit-pet\\annotations.tar.gz to data\\oxford-iiit-pet\n",
      "Preprocessing dataset...\n",
      "Processing dataset...\n",
      "Training samples: 2944\n",
      "Testing samples: 736\n",
      "Using device: cpu\n",
      "\n",
      "Training CNN...\n",
      "Epoch [1/5], Batch [1/92], Loss: 3.6239\n",
      "Epoch [1/5], Batch [11/92], Loss: 3.5608\n",
      "Epoch [1/5], Batch [21/92], Loss: 3.6179\n",
      "Epoch [1/5], Batch [31/92], Loss: 3.6096\n",
      "Epoch [1/5], Batch [41/92], Loss: 3.6215\n",
      "Epoch [1/5], Batch [51/92], Loss: 3.5956\n",
      "Epoch [1/5], Batch [61/92], Loss: 3.5698\n",
      "Epoch [1/5], Batch [71/92], Loss: 3.5727\n",
      "Epoch [1/5], Batch [81/92], Loss: 3.5448\n",
      "Epoch [1/5], Batch [91/92], Loss: 3.4910\n",
      "Epoch [1/5] Loss: 3.5913\n",
      "Epoch [2/5], Batch [1/92], Loss: 3.4545\n",
      "Epoch [2/5], Batch [11/92], Loss: 3.3515\n",
      "Epoch [2/5], Batch [21/92], Loss: 3.5078\n",
      "Epoch [2/5], Batch [31/92], Loss: 3.3839\n",
      "Epoch [2/5], Batch [41/92], Loss: 3.1974\n",
      "Epoch [2/5], Batch [51/92], Loss: 3.2984\n",
      "Epoch [2/5], Batch [61/92], Loss: 3.2404\n",
      "Epoch [2/5], Batch [71/92], Loss: 3.2992\n",
      "Epoch [2/5], Batch [81/92], Loss: 3.3588\n",
      "Epoch [2/5], Batch [91/92], Loss: 3.3547\n",
      "Epoch [2/5] Loss: 3.3860\n",
      "Epoch [3/5], Batch [1/92], Loss: 3.3486\n",
      "Epoch [3/5], Batch [11/92], Loss: 3.3693\n",
      "Epoch [3/5], Batch [21/92], Loss: 3.2572\n",
      "Epoch [3/5], Batch [31/92], Loss: 3.4337\n",
      "Epoch [3/5], Batch [41/92], Loss: 3.1799\n",
      "Epoch [3/5], Batch [51/92], Loss: 3.2168\n",
      "Epoch [3/5], Batch [61/92], Loss: 3.2711\n",
      "Epoch [3/5], Batch [71/92], Loss: 3.1015\n",
      "Epoch [3/5], Batch [81/92], Loss: 2.9850\n",
      "Epoch [3/5], Batch [91/92], Loss: 3.6247\n",
      "Epoch [3/5] Loss: 3.1950\n",
      "Epoch [4/5], Batch [1/92], Loss: 2.9343\n",
      "Epoch [4/5], Batch [11/92], Loss: 2.8242\n",
      "Epoch [4/5], Batch [21/92], Loss: 2.9552\n",
      "Epoch [4/5], Batch [31/92], Loss: 2.9977\n",
      "Epoch [4/5], Batch [41/92], Loss: 3.1097\n",
      "Epoch [4/5], Batch [51/92], Loss: 2.7873\n",
      "Epoch [4/5], Batch [61/92], Loss: 2.8082\n",
      "Epoch [4/5], Batch [71/92], Loss: 2.7542\n",
      "Epoch [4/5], Batch [81/92], Loss: 3.1759\n",
      "Epoch [4/5], Batch [91/92], Loss: 2.5882\n",
      "Epoch [4/5] Loss: 2.9077\n",
      "Epoch [5/5], Batch [1/92], Loss: 2.5013\n",
      "Epoch [5/5], Batch [11/92], Loss: 2.4729\n",
      "Epoch [5/5], Batch [21/92], Loss: 2.4585\n",
      "Epoch [5/5], Batch [31/92], Loss: 2.5327\n",
      "Epoch [5/5], Batch [41/92], Loss: 2.5213\n",
      "Epoch [5/5], Batch [51/92], Loss: 2.9273\n",
      "Epoch [5/5], Batch [61/92], Loss: 2.4189\n",
      "Epoch [5/5], Batch [71/92], Loss: 2.3872\n",
      "Epoch [5/5], Batch [81/92], Loss: 2.2890\n",
      "Epoch [5/5], Batch [91/92], Loss: 2.4844\n",
      "Epoch [5/5] Loss: 2.4304\n",
      "\n",
      "Evaluating CNN...\n",
      "Test Loss: 3.1457, Accuracy: 16.17%\n",
      "\n",
      "Training U-Net...\n",
      "Epoch [1/5], Batch [1/92], Loss: 0.7377\n",
      "Epoch [1/5], Batch [11/92], Loss: 0.1236\n",
      "Epoch [1/5], Batch [21/92], Loss: 0.0810\n",
      "Epoch [1/5], Batch [31/92], Loss: 0.0566\n",
      "Epoch [1/5], Batch [41/92], Loss: 0.0542\n",
      "Epoch [1/5], Batch [51/92], Loss: 0.0508\n",
      "Epoch [1/5], Batch [61/92], Loss: 0.0480\n",
      "Epoch [1/5], Batch [71/92], Loss: 0.0484\n",
      "Epoch [1/5], Batch [81/92], Loss: 0.0449\n",
      "Epoch [1/5], Batch [91/92], Loss: 0.0448\n",
      "Epoch [1/5] Loss: 0.0918\n",
      "Epoch [2/5], Batch [1/92], Loss: 0.0433\n",
      "Epoch [2/5], Batch [11/92], Loss: 0.0447\n",
      "Epoch [2/5], Batch [21/92], Loss: 0.0436\n",
      "Epoch [2/5], Batch [31/92], Loss: 0.0448\n",
      "Epoch [2/5], Batch [41/92], Loss: 0.0448\n",
      "Epoch [2/5], Batch [51/92], Loss: 0.0426\n",
      "Epoch [2/5], Batch [61/92], Loss: 0.0426\n",
      "Epoch [2/5], Batch [71/92], Loss: 0.0432\n",
      "Epoch [2/5], Batch [81/92], Loss: 0.0432\n",
      "Epoch [2/5], Batch [91/92], Loss: 0.0426\n",
      "Epoch [2/5] Loss: 0.0432\n",
      "Epoch [3/5], Batch [1/92], Loss: 0.0427\n",
      "Epoch [3/5], Batch [11/92], Loss: 0.0419\n",
      "Epoch [3/5], Batch [21/92], Loss: 0.0421\n",
      "Epoch [3/5], Batch [31/92], Loss: 0.0421\n",
      "Epoch [3/5], Batch [41/92], Loss: 0.0422\n",
      "Epoch [3/5], Batch [51/92], Loss: 0.0422\n",
      "Epoch [3/5], Batch [61/92], Loss: 0.0420\n",
      "Epoch [3/5], Batch [71/92], Loss: 0.0425\n",
      "Epoch [3/5], Batch [81/92], Loss: 0.0421\n",
      "Epoch [3/5], Batch [91/92], Loss: 0.0429\n",
      "Epoch [3/5] Loss: 0.0424\n",
      "Epoch [4/5], Batch [1/92], Loss: 0.0423\n",
      "Epoch [4/5], Batch [11/92], Loss: 0.0427\n",
      "Epoch [4/5], Batch [21/92], Loss: 0.0422\n",
      "Epoch [4/5], Batch [31/92], Loss: 0.0419\n",
      "Epoch [4/5], Batch [41/92], Loss: 0.0426\n",
      "Epoch [4/5], Batch [51/92], Loss: 0.0422\n",
      "Epoch [4/5], Batch [61/92], Loss: 0.0422\n",
      "Epoch [4/5], Batch [71/92], Loss: 0.0424\n",
      "Epoch [4/5], Batch [81/92], Loss: 0.0422\n",
      "Epoch [4/5], Batch [91/92], Loss: 0.0429\n",
      "Epoch [4/5] Loss: 0.0423\n",
      "Epoch [5/5], Batch [1/92], Loss: 0.0423\n",
      "Epoch [5/5], Batch [11/92], Loss: 0.0421\n",
      "Epoch [5/5], Batch [21/92], Loss: 0.0425\n",
      "Epoch [5/5], Batch [31/92], Loss: 0.0424\n",
      "Epoch [5/5], Batch [41/92], Loss: 0.0422\n",
      "Epoch [5/5], Batch [51/92], Loss: 0.0411\n",
      "Epoch [5/5], Batch [61/92], Loss: 0.0416\n",
      "Epoch [5/5], Batch [71/92], Loss: 0.0424\n",
      "Epoch [5/5], Batch [81/92], Loss: 0.0422\n",
      "Epoch [5/5], Batch [91/92], Loss: 0.0422\n",
      "Epoch [5/5] Loss: 0.0423\n",
      "\n",
      "Evaluating U-Net...\n",
      "Test Loss: 0.0423\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize data processor\n",
    "    data_processor = DataProcess(resize=(128, 128))\n",
    "\n",
    "    print(\"Loading Oxford-IIIT Pet Dataset...\")\n",
    "    pet_dataset = data_processor.load_oxford_pet()\n",
    "\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    try:\n",
    "        (train_images, test_images,\n",
    "         train_labels, test_labels,\n",
    "         train_masks, test_masks) = data_processor.preprocess_oxford_pet(pet_dataset)\n",
    "\n",
    "        # Create dataset objects\n",
    "        train_dataset = PetDataset(train_images, train_masks, train_labels)\n",
    "        test_dataset = PetDataset(test_images, test_masks, test_labels)\n",
    "\n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "        print(f\"Training samples: {len(train_dataset)}\")\n",
    "        print(f\"Testing samples: {len(test_dataset)}\")\n",
    "\n",
    "        # Initialize device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        # Initialize models\n",
    "        cnn_model = CNN(num_classes=37).to(device)  # 37 classes for Oxford Pet dataset\n",
    "        unet_model = UNet().to(device)\n",
    "\n",
    "        # Define loss functions and optimizers\n",
    "        criterion_cls = nn.CrossEntropyLoss()\n",
    "        criterion_seg = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        optimizer_cnn = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "        optimizer_unet = optim.Adam(unet_model.parameters(), lr=0.001)\n",
    "\n",
    "        # Train and evaluate CNN\n",
    "        print(\"\\nTraining CNN...\")\n",
    "        train_model(cnn_model, train_loader, criterion_cls, optimizer_cnn, device)\n",
    "        print(\"\\nEvaluating CNN...\")\n",
    "        evaluate_model(cnn_model, test_loader, criterion_cls, device)\n",
    "\n",
    "        # Train and evaluate U-Net\n",
    "        print(\"\\nTraining U-Net...\")\n",
    "        train_unet(unet_model, train_loader, criterion_seg, optimizer_unet, device)\n",
    "        print(\"\\nEvaluating U-Net...\")\n",
    "        evaluate_unet(unet_model, test_loader, criterion_seg, device)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b48c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
